\section{Introduction}

~\cite{a}
We present a novel feedback neural networks for joint reasoning the class node and hidden layer information. The network is powerful to be applied on model class visualization and object localization even in cluttered scenes with multi objects. The framework is novel and 

\textbf{Deep Learning and Deep Convolutional Neural Networks, Feedforward Strcture}
Deep Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale im- age and video recognition (Krizhevsky et al., 2012; Zeiler Fergus, 2013; Sermanet et al., 2014; Simonyan Zisserman, 2014) which has become possible due to the large public image reposito- ries, such as ImageNet (Deng et al., 2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al., 2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recog- nition Challenge (ILSVRC) (Russakovsky et al., 2014), which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature en- codings (Perronnin et al., 2010) (the winner of ILSVRC-2011) to deep ConvNets (Krizhevsky et al., 2012) (the winner of ILSVRC-2012). 

\textbf{Psychological feedback, inference top-down and bottom-up}
While we have outlined in this paper a hierarchical feedfor- ward view on visual processing, it is important to remember that within the visual cortex there are generally more feedback connections than forward connections. Also lateral connec- tions play an important role. This hints at the importance of processes like attention, expectation, top-down reasoning, imagination, and filling in. Many computer vision systems try to work in a purely feed-forward fashion. However, vision is inherently ambiguous and benefits from any prior knowledge available. This may even imply that the knowledge of how the tower of Pisa looks influences the perception of an edge on the level of V1. It also means that a system should be able to produce several hypotheses that are concurrently considered and possibly not resolved [102]. 

\textbf{This paper Main Contribution}
In this paper, we address the visualisation of deep image classification ConvNets, trained on the large-scale ImageNet challenge dataset [2]. To this end, we make the following three contributions. First, we demonstrate that understandable visualisations of ConvNet classification models can be ob- tained using the numerical optimisation of the input image [5] (Sect. 2). Note, in our case, unlike [5], the net is trained in a supervised manner, so we know which neuron in the final fully-connected clas- sification layer should be maximised to visualise the class of interest (in the unsupervised case, [9] had to use a separate annotated image set to find out the neuron responsible for a particular class). To the best of our knowledge, we are the first to apply the method of [5] to the visualisation of ImageNet classification ConvNets [8]. Second, we propose a method for computing the spatial support of a given class in a given image (image-specific class saliency map) using a single back-propagation pass through a classification ConvNet (Sect. 3). As discussed in Sect. 3.2, such saliency maps can be used for weakly supervised object localisation. Finally, we show in Sect. 4 that the gradient-based visualisation methods generalise the deconvolutional network reconstruction procedure [13]. 

\textbf{Yurgen's feedback neural networks, Attention neural networks, Deep Boltzman Machines}

\textbf{DPM  Top-down, weakly supervised object detection, localization and parsing}
We describe an object detection system that represents highly variable objects using mixtures of multiscale de- formable part models. These models are trained using a discriminative procedure that only requires bounding boxes for the objects in a set of images. The resulting system is both efficient and accurate, achieving state-of- the-art results on the PASCAL VOC benchmarks [11]– [13] and the INRIA Person dataset [10]. 
Our approach builds on the pictorial structures frame- work [15], [20]. Pictorial structures represent objects by a collection of parts arranged in a deformable configu- ration. Each part captures local appearance properties of an object while the deformable configuration is charac- terized by spring-like connections between certain pairs of parts. 
Detections obtained with a single component person model. The model is defined by a coarse root filter (a), several higher resolution part filters (b) and a spatial model for the location of each part relative to the root (c). The filters specify weights for histogram of oriented gradients features. Their visualization show the positive weights at different orientations. The visualization of the spatial models reflects the “cost” of placing the center of a part at different locations relative to the root. 

\textbf{Comparing against Oxford and Deconv}

\textbf{ConvNet Implementation Details}
ConvNet implementation details. Our visualisation experiments were carried out using a single deep ConvNet, trained on the ILSVRC-2013 dataset [2], which includes 1.2M training images, labelled into 1000 classes. Our ConvNet is similar to that of [8] and is implemented using their cuda-convnet toolbox1, although our net is less wide, and we used additional image jittering, based on zeroing-out random parts of an image. Our weight layer configuration is: conv64-conv256- conv256-conv256-conv256-full4096-full4096-full1000, where convN denotes a convolutional layer with N filters, fullM – a fully-connected layer with M outputs. On ILSVRC-2013 validation set, the network achieves the top-1/top-5 classification error of 39.7%/17.7%, which is slightly better than 40.7%/18.2%, reported in [8] for a single ConvNet.

\setlength{\tabcolsep}{2pt}
\begin{figure}
\begin{center}
\includegraphics[width=0.95\columnwidth]{figs/splash0/splash0}
% \vspace{-10pt}
\caption{We propose a novel feedback convnet for weakly supervised object localization in complex scenes with cluttered background. The feedback net is able to utilized both bottom-up image features and top-down semantic labels to infer the hidden layer neuron status to match the localize the correponding salient area in the image. }
\label{fig:splash0}
% \vspace{-30pt}
\end{center}
\end{figure}

\setlength{\tabcolsep}{2pt}
\begin{figure*}
\begin{center}
\begin{tabular}{ccccccc}
%\rotatebox{90}{\hspace{5mm}Sequential} &
\includegraphics[width=0.13\linewidth]{figs/splash/original} &
\includegraphics[width=0.13\linewidth]{figs/splash/panda} &
\includegraphics[width=0.13\linewidth]{figs/splash/tiger} &
\includegraphics[width=0.13\linewidth]{figs/splash/gorilla} &
\includegraphics[width=0.13\linewidth]{figs/splash/lion} &
\includegraphics[width=0.13\linewidth]{figs/splash/elephant} &
\includegraphics[width=0.13\linewidth]{figs/splash/localization}\\
{\small (a) Input Image} &
{\small (b) Panda} &
{\small (c) Tiger} &
{\small (d) Gorilla} &
{\small (e) Lion} &
{\small (f) Elephant} &
{\small (g) Localization}
\end{tabular}
% \vspace{-10pt}
\caption{We illustrate the localization power of the feedback net on a multi-object image with cluttered background. (a) shows the original input image which both VggNet and GoogleNet recongize as "comic". (b) - (f) demonstrate the powerfulness of our model understanding the image given paticular object labels. We visualization the gradient of each label w.r.t. image after ther convergence of the feedback neural nets (g) shows the localization power for different objects in this complex image based on the gradient. Note that the weights in the net is obtained from a pre-trained feedforward GoogleNet model for image classification.}
\label{fig:splah}
% \vspace{-30pt}
\end{center}
\end{figure*}

\setlength{\tabcolsep}{2pt}
\begin{figure*}
\begin{center}
\begin{tabular}{ccccccccc}
\rotatebox{90}{\hspace{5mm}Oxford} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/gradient/oxford/panda} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/saliency/oxford/panda} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/gradient/oxford/tiger} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/saliency/oxford/tiger} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/gradient/oxford/gorilla} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/saliency/oxford/gorilla} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/gradient/oxford/lion} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/saliency/oxford/lion} \\
\rotatebox{90}{\hspace{5mm}Deconv} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/gradient/deconv/panda} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/saliency/deconv/panda} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/gradient/deconv/tiger} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/saliency/deconv/tiger} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/gradient/deconv/gorilla} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/saliency/deconv/gorilla} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/gradient/deconv/lion} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/saliency/deconv/lion} \\
\rotatebox{90}{\hspace{5mm}Our} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/gradient/feedback/panda} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/saliency/feedback/panda} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/gradient/feedback/tiger} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/saliency/feedback/tiger} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/gradient/feedback/gorilla} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/saliency/feedback/gorilla} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/gradient/feedback/lion} &
\includegraphics[width=0.11\linewidth]{figs/visual_compare/saliency/feedback/lion} \\
&
\multicolumn{2}{c}{{\small (a) Panda}} &
\multicolumn{2}{c}{{\small (b) Tiger}} &
\multicolumn{2}{c}{{\small (c) Gorilla}} &
\multicolumn{2}{c}{{\small (d) Lion}} \\
\end{tabular}
% \vspace{-10pt}
\caption{We demonstrate the effectiveness of our method by comparing the class model visualization results against Oxford [] and Deconv []. The input image is the same as Figure 1 (a). We show both the visualization results as well as the saliency map. While both Oxford and Deconv have the same input: the image and an object class label (i.e. tiger, panda, etc.), the gradients computed are often salient on one particular object (i.e. elephant). Our feedback framework allows for the model to focus on the most important image area that improves the class confidence.}
\label{fig:visual_compare}
% \vspace{-30pt}
\end{center}
\end{figure*}


