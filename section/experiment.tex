\section{Experiment Results}
\label{sec:experiment}

\subsection{Image Specific Class Model Visualization and Visual Saliency Extraction}
Given an image $I$, a class label $k$ and the hidden neuron activation states $h$, we approximate the neural net class score $S_k$ with the first-order taylor expansion in the neighborhood of $I$:
\begin{equation}
  S_k(I, h) \approx  T_k(h)^T I + b
\end{equation}
where $T_k(h)$ is the derivative of $S_k$ with respect to the image at the point of $I$ and hidden neuron status $h$.

$T_k(h)$ can be also viewed as the linear template applied on image $I$ for the understanding of how like the image belongs to class $k$. We can visualize $T$ since it's the same size as the image $I$. We use this technique to visualize the model through the paper.

Specifically, for a VggNet~\cite{Simonyan2014Very} which uses only a stack of piece-wise linear layers (i.e.\ convolution, relu, max-pooling) to compute the class scores, once the hidden states of these layers are given, the final score is an linear operation on the image.

As pointed in~\cite{simonyan2013deep}, the magnitude of the elements in the model template $T$ defines the class specific salience map for image $I$. A pixel with large magnitude indicates that it is more important to the class $k$. We adopt the same saliency extraction strategy as~\cite{simonyan2013deep} that a single class saliency value $M$ at pixel $(i,j)$ is computed across all color channels: $M_{ij} = \max_{c \in rgb} | T(i,j,c) |$.

\textbf{Comparison of Visualization Methods:} We compare the visualization and saliency extraction of the feedbacked results against~\cite{simonyan2013deep} (Oxford) and~\cite{zeiler2014visualizing} (Deconv) on a set of complex images containing multiple objects from different classes. We show the qualititative results in Fig.~\ref{fig:examples}. All techniques use the same pre-trained VggNet\cite{Simonyan2014Very} and ground truth class labels for each image are given. The visualization results before feedback is the same as Oxford, where all the hidden neurons states are determined by bottom-up computation. The visualization results after the feedback process are similar to Deconv, except that our model captures the most salient visual regions for each specific class. 

\textbf{Comparison of ConvNet Models:} We compare the three most popluar ConvNets: AlexNet~\cite{Krizhevsky2012ImageNet}, VggNet~\cite{Simonyan2014Very} and Googlenet~\cite{Szegedy2014Going} by visualizing their feedback templates in Fig.~\ref{fig:model_compare}. All models are pre-trained on Caffe with the same classificatino accuracy reported. Ground truth class labels are given for each image before feedback. From the visualization results, we find that VggNet and GoogleNet produce more accurate visual attention than AlexNet, suggesting the benefit of using smaller convolution filters and deeper architectures to further distinguish similar and closeby objects. We also observe that, although both VggNet and GoogleNet produce very smilar image classification accuracies, GoogleNet better captures the salient object areas for each class than VggNet. We hypothesize that the two 4096 dimensional fully connected layers (i.e.\ fc6, fc7) in VggNet (which GoogleNet does not contain) could harm the spatial distinctiveness of the final image features,  

\textbf{Feedback Attention for Fine-Grained Classification:} We show some interesting feedback visualization results for understanding the fine-grained classifcation ability of VggNet~\cite{Simonyan2014Very}. The VggNet is trained on ImageNet dataset~\cite{deng2009imagenet} with 1000 object classes which contains $\sim$100 fine-graiend dog categories and $\sim$500 animal categories. We visualize the feedback templates of a few dog-cat images with respect to some dog classes and animal classes in Fig.~\ref{fig:class_compare}. We observe that each class has its own special salient image features for distinction. Some features correpsond to parts (i.e.\ nose and ears) while some others correpsond to attributes (i.e.\ furry and tabby).

\subsection{Weakly Supervised Object Localization}
One could expect that pixels with large magnitude locate around the corresponding object in the image. 
Once we extract the salience map, we could use it for weakly supervised object localization, where we are only utilizing a pre-trained classification ConvNet and an image label. We follow~\cite{xxx}

Given an image and the corresponding class saliency map, we compute the object segmentation mask
using the GraphCut colour segmentation [3]. The use of the colour segmentation is motivated by the
fact that the saliency map might capture only the most discriminative part of an object, so saliency
thresholding might not be able to highlight the whole object. Therefore, it is important to be able
to propagate the thresholded map to other parts of the object, which we aim to achieve here using
the colour continuity cues. Foreground and background colour models were set to be the Gaussian
Mixture Models. The foreground model was estimated from the pixels with the saliency higher than
a threshold, set to the 95% quantile of the saliency distribution in the image; the background model
was estimated from the pixels with the saliency smaller than the 30% quantile (Fig. 3, right-middle).
The GraphCut segmentation [3] was then performed using the publicly available implementation2 .
Once the image pixel labelling into foreground and background is computed, the object segmentation
mask is set to the largest connected component of the foreground pixels (Fig. 3, right).

We entered our object localisation method into the ILSVRC-2013 localisation challenge. Consid-
ering that the challenge requires the object bounding boxes to be reported, we computed them as
the bounding boxes of the object segmentation masks. The procedure was repeated for each of the
top-5 predicted classes. The method achieved 46.4% top-5 error on the test set of ILSVRC-2013.
It should be noted that the method is weakly supervised (unlike the challenge winner with 29.9%
error), and the object localisation task was not taken into account during training. In spite of its
simplicity, the method still outperformed our submission to ILSVRC-2012 challenge (which used
the same dataset), which achieved 50.0% localisation error using a fully-supervised algorithm based
on the part-based models [6] and Fisher vector feature encoding [11].

\begin{table}
\centering
Evaluation on Imagenet 2014 localization test set
\begin{tabular}{|c|c|c|}
\hline
Method & Localization error & Classification error\\
\hline
VggNet-Supervised[] & 25.3 & 7.4 \\
GoogleNet-Supervised [] & 26.4 & 14.8 \\
AlexNet-Weakly[] & & \\
AlexNet-Weakly Feedback & & \\
VggNet-Weakly Feedback & & \\
GooglNet-Weakl Feedback & & \\
\hline
\end{tabular}
\caption{We show the localization evaluation on ImageNet 2014 localization competition. Our model clearly outperforms the weakly supervised approach based on []. Notably, we compare even fairbly well against supervisedly trained localization model, where an extra localization bouding boxes dataset is used. We demonstrate that when learning for classifications objectives, the Deep ConvNet already integrate powerful class specific features for attentioning on the important areas.}
\label{tab:localization_accuracy}
\end{table}

\setlength{\tabcolsep}{2pt}
\begin{figure*}
\begin{center}
\begin{tabular}{ccccccc}
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat1_sali_258} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat1_diff_258} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/dog-cat1_diff_258} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/dog-cat1} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/dog-cat1_diff_286} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat1_diff_286} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat1_sali_286} \\
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat2_sali_163} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat2_diff_163} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/dog-cat2_diff_163} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/dog-cat2} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/dog-cat2_diff_286} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat2_diff_286} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat2_sali_286} \\
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat3_sali_188} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat3_diff_188} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/dog-cat3_diff_188} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/dog-cat3} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/dog-cat3_diff_286} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat3_diff_286} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat3_sali_286} \\
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat4_sali_243} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat4_diff_243} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/dog-cat4_diff_243} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/dog-cat4} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/dog-cat4_diff_286} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat4_diff_286} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/dog-cat4_sali_286} \\
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car1_sali_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car1_diff_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/bic-car1_diff_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/bic-car1} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/bic-car1_diff_672} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car1_diff_672} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car1_sali_672} \\
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car2_sali_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car2_diff_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/bic-car2_diff_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/bic-car2} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/bic-car2_diff_672} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car2_diff_672} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car2_sali_672} \\
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele1_sali_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele1_diff_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/zeb-ele1_diff_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/zeb-ele1} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/zeb-ele1_diff_387} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele1_diff_387} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele1_sali_387} \\
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele2_sali_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele2_diff_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/zeb-ele2_diff_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/zeb-ele2} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/oxford/zeb-ele2_diff_387} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele2_diff_387} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele2_sali_387} \\
{\small (a) Saliency} &
{\small (b) Feedback} &
{\small (c) Gradient} &
{\small (d) Image} &
{\small (e) Gradient} &
{\small (f) Feedback} &
{\small (g) Saliency}
\end{tabular}
% \vspace{-10pt}
\caption{We illustrate the effectiveness of feedback neural networks (built on a pre-trained GoogleNet) for class-specific feature extraction. Column (d) shows the selected input images containing two different objects (i.e.\ dog v.s. cat, car v.s. bike, and zebra v.s. elephant). Column (c) and (e) show the orignal image gradients of the two different classes. Column (b) and (f) show the image gradients after feedback net finishes adjusting hidden neuron activiations w.r.t. the two different classes. Column (a) and (g) show the salience map computed from the gradients after feedback. Comparing against the orignal image gradients in (c) and (e) which usually spread over the entire image, feedbacked gradients in (b) and (f) focus more on the corresponding object area. Better viewed in color.}
\label{fig:examples}
% \vspace{-30pt}
\end{center}
\end{figure*}


\setlength{\tabcolsep}{2pt}
\begin{figure*}
\begin{center}
\begin{tabular}{cccccccc}
\rotatebox{90}{\hspace{5mm}Gradient} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/zeb-ele1_diff_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/zeb-ele1_diff_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele1_diff_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele1} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/zeb-ele1_diff_387} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/zeb-ele1_diff_387} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele1_diff_387} \\
\rotatebox{90}{\hspace{5mm}Saliency} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/zeb-ele1_sali_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/zeb-ele1_sali_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele1_sali_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele1} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/zeb-ele1_sali_387} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/zeb-ele1_sali_387} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele1_sali_387} \\
\rotatebox{90}{\hspace{5mm}Gradient} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/zeb-ele2_diff_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/zeb-ele2_diff_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele2_diff_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele2} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/zeb-ele2_diff_387} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/zeb-ele2_diff_387} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele2_diff_387} \\
\rotatebox{90}{\hspace{5mm}Saliency} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/zeb-ele2_sali_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/zeb-ele2_sali_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele2_sali_341} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele2} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/zeb-ele2_sali_387} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/zeb-ele2_sali_387} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/zeb-ele2_sali_387} \\
\rotatebox{90}{\hspace{5mm}Gradient} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/bic-car1_diff_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/bic-car1_diff_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car1_diff_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car1} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/bic-car1_diff_672} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/bic-car1_diff_672} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car1_diff_672} \\
\rotatebox{90}{\hspace{5mm}Saliency} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/bic-car1_sali_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/bic-car1_sali_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car1_sali_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car1} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/bic-car1_sali_672} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/bic-car1_sali_672} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car1_sali_672} \\
\rotatebox{90}{\hspace{5mm}Gradient} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/bic-car2_diff_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/bic-car2_diff_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car2_diff_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car2} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/bic-car2_diff_672} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/bic-car2_diff_672} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car2_diff_672} \\
\rotatebox{90}{\hspace{5mm}Saliency} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/bic-car2_sali_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/bic-car2_sali_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car2_sali_818} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car2} &
\includegraphics[width=0.13\linewidth]{figs/examples/alexnet/soft/bic-car2_sali_672} &
\includegraphics[width=0.13\linewidth]{figs/examples/vggnet/soft/bic-car2_sali_672} &
\includegraphics[width=0.13\linewidth]{figs/examples/googlenet/soft/bic-car2_sali_672} \\
&
{\small (a) AlexNet} &
{\small (b) VggNet} &
{\small (c) GoogleNet} &
{\small (d) Image} &
{\small (e) AlexNet} &
{\small (f) VggNet} &
{\small (g) GoogleNet}
\end{tabular}
% \vspace{-10pt}
\caption{We visualize the feedback ability of three most popular pre-trained ConvNets: AlexNet, VggNet and GoogleNet. We show the input images in column (d). We show the feedbacked image gradients and salience maps for each image. On the left 3 columns, we compute the results w.r.t. zebra or car, on the right 3 columns we compute the results w.r.t. elephant or bike. In the visualization results, VggNet performs quite better than AlexNet, especially in capturing the most salient object details, suggesting the benefit of usage of small convolutional filters and deeper structures. Although both VggNet and GogoleNet produce similar classification accruacy, we find GoogleNet provides the better class specific feature separations. We suspect the two 4096 fully connected layers in VggNet (which GoogleNetdoes not have) could harm the spatial distinctiveness of image features.}
\label{fig:model_compare}
% \vspace{-30pt}
\end{center}
\end{figure*}

\setlength{\tabcolsep}{2pt}
\begin{figure*}
\begin{center}
\begin{tabular}{ccccccc}
{\small (a) Image} &
{\small (b) G. pyrenees} &
{\small (c) Beagle} &
{\small (d) York. terrier} &
{\small (e) Kit fox} &
{\small (f) Tiger} &
{\small (g) Ostrich} \\
&
\includegraphics[width=0.13\linewidth]{figs/class_compare/pyrenees} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/beagle} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/yorkshire-terrier} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/kit-fox} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/tiger} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/ostrich} \\
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat1} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat1_diff_258} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat1_diff_163} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat1_diff_188} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat1_diff_279} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat1_diff_293} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat1_diff_10} \\
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat2} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat2_diff_258} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat2_diff_163} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat2_diff_188} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat2_diff_279} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat2_diff_293} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat2_diff_10} \\
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat3} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat3_diff_258} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat3_diff_163} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat3_diff_188} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat3_diff_279} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat3_diff_293} &
\includegraphics[width=0.13\linewidth]{figs/class_compare/googlenet/soft/dog-cat3_diff_10} \\
\end{tabular}
% \vspace{-10pt}
\caption{We show some interesting visualizations for the understanding of fine-grained classification by comparsing against the feedback gradients of ground truth labels and other classes. The top row shows the class labels and a representative image for each class for the ease of understanding. Column (a) shows the three examplar input images, their ground truth labels are great pyrenees, beagle and yorkshire terrier respectively. We can see that although (b), (c) and (d) are all dogs, their salient area for distinction are quite different. Noses are one of the most important feature for classifying dogs, but ears are specific feature for beagles, while fluffy is more importatnt to yorkshire terrier. When the top down is from (e) kit fox, features on the cat in the last row is more fox-specific: nose and ears. When top down is from (f) tiger, features on the same at is more tiger-specific: textures. And when it's (g) ostrich, nothing special come out.}
\label{fig:class_compare}
% \vspace{-30pt}
\end{center}
\end{figure*}
